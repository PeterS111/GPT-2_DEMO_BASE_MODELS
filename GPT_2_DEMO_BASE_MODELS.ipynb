{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2_DEMO_BASE_MODELS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1t7JmkuZKPNpO8qIpVrG_0hyuO4PbJpOk",
      "authorship_tag": "ABX9TyNizhslzUda3hWFGHCrikl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterS111/GPT-2_DEMO_BASE_MODELS/blob/main/GPT_2_DEMO_BASE_MODELS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCyyVBobiKx"
      },
      "source": [
        "# GPT-2 Base Models Demo\r\n",
        "version 10.12.2020\r\n",
        "\r\n",
        "The code is based on https://github.com/nshepperd/gpt-2 by N Shepperd, with small changes by Peter S  \r\n",
        "\r\n",
        "\r\n",
        "This notebook allows to generate text from \"Base\" GPT-2 models - the models which were NOT fine-tuned. There are four models (\"M\" stands for Millions of parameters):\r\n",
        "\r\n",
        "GPT-2 Small: 124M, \r\n",
        "GPT-2 Medium: 345M, \r\n",
        "GPT-2 Large: 774M,\r\n",
        "GPT-2 XLarge: 1558M\r\n",
        "\r\n",
        "First make sure that you are using a GPU: Runtime/Change runtime type/ -> Select \"GPU\". There are four types of GPUs on Colab: K80, P4, T4 and P100. If you want to run GPT-2 XLarge you will need T4 or P100. For other models any GPU will work. To check which GPU you have, run the command below. If the GPU is not the one you require go to \"Runtime/Factory reset runtime\" and run the command below again. Usually after a few \"resets\" you will get either T4 or P100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeSNbfZkb2Sh"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ffm4SUefQ8x"
      },
      "source": [
        "1. Download the repository from github:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4BKKt01Wzwt"
      },
      "source": [
        "!git clone https://github.com/PeterS111/GPT-2_DEMO_BASE_MODELS/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_THwi1YcfZrJ"
      },
      "source": [
        "2. Run the following command to ensure you have TensorFlow v 1.x. This code doesn't work with versions 2.x "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVN9fGFhyDJ"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn6s0rVobB2i"
      },
      "source": [
        "3. Install the requirements. After installation you will get the message that \"You must restart the runtime in order to use newly installed versions.\" DON'T DO IT!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgvL8mMGWz_w"
      },
      "source": [
        "!pip3 install -r /content/GPT-2_DEMO_BASE_MODELS/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ2cP9xBg4A6"
      },
      "source": [
        "4. Download the GPT-2 model. The command below will download the GPT-2 Small: 124M parameters. If you want other version, change the parameter to what you require. You can choose from the following: 124M, 345M, 774M and 1558M. Keep in mind than for GPT-2 XLarge you will need GPU either T4 or P100. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c40YoqpxW0dZ"
      },
      "source": [
        "!python /content/GPT-2_DEMO_BASE_MODELS/download_model.py 124M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QIu4XzWhqk6"
      },
      "source": [
        "5. Move the downloaded model to the main folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vU0PQpqApI"
      },
      "source": [
        "!mv -v  /content/models/* /content/GPT-2_DEMO_BASE_MODELS/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Akh0Gzxh2dR"
      },
      "source": [
        "6. Change the working directory to the main folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0L_hES62q3"
      },
      "source": [
        "cd /content/GPT-2_DEMO_BASE_MODELS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_I5KuAhiQ3p"
      },
      "source": [
        "# 7. Run the models!\r\n",
        "\r\n",
        "There are three ways you can run the model:\r\n",
        "\r\n",
        "First of the commands below will run the model in an interactive manner. You will have to enter a \"promt\" (which can be any piece of continuous text) and press enter. Generated text will appear below.\r\n",
        "\r\n",
        "Second command will generate text to file, which can be found in \"outputs\" directory. You will have to enter the prompt in the last argument, like this: --raw_text \"It's nice weather today\"\r\n",
        "\r\n",
        "Third command will generate random text to file. No prompt here.\r\n",
        "\r\n",
        "The \"--nsamples\" parameter is how many samples will be generated.\r\n",
        "\r\n",
        "Make sure sure that \"--model_name\" parameter matches the model you downloaded!\r\n",
        "\r\n",
        "The maximum length you can choose is 1024.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oxiojGM6xj4"
      },
      "source": [
        "!python interactive_conditional_samples.py --model_name=124M --nsamples=1 --batch_size=1 --length=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZRlAD6fPJd4"
      },
      "source": [
        "!python generate_conditional_samples_to_file.py --model_name=124M --length 300 --seed 50 --temperature 1.0 --top_k 50 --top_p 1.0 --nsamples 1 --raw_text \"Let's try\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J83yubJeJ3gx"
      },
      "source": [
        "!python generate_unconditional_samples_to_file.py --model_name=124M --length 300 --seed 50 --temperature 1.0 --top_k 50 --top_p 1.0 --nsamples 5 "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}