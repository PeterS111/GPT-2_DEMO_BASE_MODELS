{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2_DEMO_BASE_MODELS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCyyVBobiKx"
      },
      "source": [
        "# GPT-2 Base Models Demo\r\n",
        "version 10.12.2020\r\n",
        "\r\n",
        "The code is based on https://github.com/nshepperd/gpt-2 by N Shepperd, with small changes by Peter S\r\n",
        "\r\n",
        "\r\n",
        "This notebook allows to generate text from \"Base\" GPT-2 models - the models which were NOT fine-tuned. There are four models (\"M\" stands for Millions of parameters):\r\n",
        "\r\n",
        "GPT-2 Small: 124M, \r\n",
        "GPT-2 Medium: 345M, \r\n",
        "GPT-2 Large: 774M,\r\n",
        "GPT-2 XLarge: 1558M\r\n",
        "\r\n",
        "First make sure that you are using a GPU: Runtime/Change runtime type/ -> Select \"GPU\". There are four types of GPUs on Colab: K80, P4, T4 and P100. If you want to run GPT-2 XLarge you will need T4 or P100. For other models any GPU will work. To check which GPU you have, run the command below. If the GPU is not the one you require go to \"Runtime/Factory reset runtime\" and run the command below again. Usually after a few \"resets\" you will get either T4 or P100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeSNbfZkb2Sh",
        "outputId": "af19ce6c-871b-4a82-ec10-579fc112deb7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec 10 12:48:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ffm4SUefQ8x"
      },
      "source": [
        "1. Download the repository from github:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4BKKt01Wzwt"
      },
      "source": [
        "!git clone https://github.com/PeterS111/GPT-2_DEMO_BASE_MODELS/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_THwi1YcfZrJ"
      },
      "source": [
        "2. Run the following command to ensure you have TensorFlow v 1.x. This code doesn't work with versions 2.x "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoVN9fGFhyDJ"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn6s0rVobB2i"
      },
      "source": [
        "\r\n",
        "3. Now install the requirements. After installation you will get the message that \"You must restart the runtime in order to use newly installed versions.\" You don't have to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgvL8mMGWz_w"
      },
      "source": [
        "!pip3 install -r /content/GPT-2_DEMO_BASE_MODELS/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK_r2VvxW0Ov"
      },
      "source": [
        "## AVAILABLE MODELS: 117M, 124M, 345M, 355M, 774M 1558M\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ2cP9xBg4A6"
      },
      "source": [
        "4. Download the GPT-2 model. The command below will download the GPT-2 Small: 124M parameters. If you want other version, change the parameter to what you require. You can choose from the following: 124M, 345M, 774M and 1558M. Keep in mind than for GPT-2 XLarge you will need GPU either T4 or P100. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c40YoqpxW0dZ"
      },
      "source": [
        "!python /content/GPT-2_DEMO_BASE_MODELS/download_model.py 124M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QIu4XzWhqk6"
      },
      "source": [
        "5. Now move the downloaded model to the main folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vU0PQpqApI"
      },
      "source": [
        "!mv -v  /content/models/* /content/GPT-2_DEMO_BASE_MODELS/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Akh0Gzxh2dR"
      },
      "source": [
        "6. Now change the working directory to the main folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0L_hES62q3"
      },
      "source": [
        "cd /content/GPT-2_DEMO_BASE_MODELS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_I5KuAhiQ3p"
      },
      "source": [
        "# Now you can run the models!\r\n",
        "First command below will run the model in an interactive manner. You will have to enter a \"promt\" (which can be any piece of continuous text) and press enter. Generated text will appear below.\r\n",
        "\r\n",
        "Second command will generate text to file, which can be found in \"outputs\" directory. You will have to enter the prompt in the last argument, like this: --raw_text \"It's nice weather today\"\r\n",
        "\r\n",
        "Third command will generate random text to file. No prompt here.\r\n",
        "\r\n",
        "The \"--nsamples\" argument is how many samples will be generated.\r\n",
        "\r\n",
        "Make sure sure that \"--model_name\" argument matches the model you downloaded!\r\n",
        "\r\n",
        "The maximum length you can choose is 1024.\r\n",
        "\r\n",
        "You can read about --top_k and --top_p arguments here:\r\n",
        "\r\n",
        "For a quick way to experiment with fine-tuning GPT-2 check this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oxiojGM6xj4"
      },
      "source": [
        "!python interactive_conditional_samples.py --model_name=124M --nsamples=1 --batch_size=1 --length=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZRlAD6fPJd4"
      },
      "source": [
        "!python generate_conditional_samples_to_file.py --model_name=124M --length 300 --seed 50 --temperature 1.0 --top_k 50 --top_p 1.0 --nsamples 1 --raw_text \"Let's try\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J83yubJeJ3gx"
      },
      "source": [
        "!python generate_unconditional_samples_to_file.py --model_name=124M --length 300 --seed 50 --temperature 1.0 --top_k 50 --top_p 1.0 --nsamples 5 "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}